{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWOT PIXC pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One file test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4423064, 14)\n",
      "    longitude   latitude  phase_noise_std  waterfrac  geolocqual        sig0  \\\n",
      "0 -149.552404  69.236015         0.045659   0.167256           0   24.465372   \n",
      "1 -149.551882  69.236147         0.031159   1.224904           0  322.846558   \n",
      "2 -149.551167  69.236328         0.026563   2.583732           0  965.866943   \n",
      "3 -149.550266  69.236555         0.022460   1.348676           0  588.309814   \n",
      "4 -149.549266  69.236808         0.023962   0.945739           0  326.267853   \n",
      "\n",
      "   sig0_uncert    crosstrack   elevation  cycle_number  pass_number  \\\n",
      "0    11.847507 -13393.489258  296.291781            35           52   \n",
      "1   151.669876 -13441.602539  295.972732            35           52   \n",
      "2   452.993774 -13492.628906  295.815024            35           52   \n",
      "3   276.064575 -13541.678711  295.813125            35           52   \n",
      "4   153.266998 -13596.972656  295.896783            35           52   \n",
      "\n",
      "   tile_number           time_granule_start  \\\n",
      "0           30  2025-07-01T10:55:09.367570Z   \n",
      "1           30  2025-07-01T10:55:09.367570Z   \n",
      "2           30  2025-07-01T10:55:09.367570Z   \n",
      "3           30  2025-07-01T10:55:09.367570Z   \n",
      "4           30  2025-07-01T10:55:09.367570Z   \n",
      "\n",
      "                                         source_file  \n",
      "0  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "1  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "2  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "3  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "4  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n"
     ]
    }
   ],
   "source": [
    "# ---------- Filepath ----------\n",
    "aufeis_test_tile_filepath = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data/SWOT_L2_HR_PIXC_033_487_278R_20250605T070551_20250605T070602_PID0_01.nc\"\n",
    "\n",
    "# NetCDF structure check\n",
    "import netCDF4\n",
    "data_structure = netCDF4.Dataset(aufeis_test_tile_filepath)\n",
    "# print(data_structure) # shows 'groups: pixel_cloud, tvp, noise'\n",
    "\n",
    "# file pointer\n",
    "fp = xr.open_dataset(aufeis_test_tile_filepath, group = 'pixel_cloud')\n",
    "# print(fp)\n",
    "# print(fp.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      longitude   latitude      height  phase_noise_std   dheight  class  \\\n",
      "211 -143.098029  68.201372  571.506653         0.117613  0.744377    6.0   \n",
      "212 -143.096199  68.200914  571.323730         0.110314  0.758199    6.0   \n",
      "213 -143.093815  68.200317  571.329895         0.131377  0.776198    3.0   \n",
      "285 -142.934672  68.160307  572.359375         0.109845  1.980290    3.0   \n",
      "286 -142.934076  68.160156  572.105469         0.061149  1.984812    3.0   \n",
      "\n",
      "     classqual  bright_land_flag  ancillary_surface_classification_flag  \\\n",
      "211        6.0               0.0                                    1.0   \n",
      "212        6.0               0.0                                    1.0   \n",
      "213        3.0               0.0                                    1.0   \n",
      "285        3.0               0.0                                    1.0   \n",
      "286        3.0               0.0                                    1.0   \n",
      "\n",
      "     waterfrac     ...        crosstrack   pixel_area  darea_dheight  \\\n",
      "211   0.888415     ...      11937.378906  1088.975708       0.005491   \n",
      "212   1.082915     ...      11958.584961  1087.053467       0.005472   \n",
      "213   0.215443     ...      11978.597656  1085.245728       0.005454   \n",
      "285   0.190502     ...      13149.444336   989.058838       0.004536   \n",
      "286   0.441215     ...      13197.599609   985.463013       0.004503   \n",
      "\n",
      "        geoid  solid_tide  load_tide  pole_tide  height_uncert  \\\n",
      "211  6.751453   -0.022910  -0.008105  -0.000049       0.087549   \n",
      "212  6.750676   -0.022910  -0.008105  -0.000049       0.083640   \n",
      "213  6.749667   -0.022911  -0.008105  -0.000049       0.101975   \n",
      "285  6.684452   -0.022986  -0.008123  -0.000052       0.217524   \n",
      "286  6.684213   -0.022986  -0.008123  -0.000052       0.121369   \n",
      "\n",
      "     geoid_correction   elevation  \n",
      "211          6.782516  564.724121  \n",
      "212          6.781741  564.541992  \n",
      "213          6.780733  564.549133  \n",
      "285          6.715611  565.643738  \n",
      "286          6.715374  565.390076  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the variables we want into 1D numpy arrays\n",
    "def getvar(name):\n",
    "    return fp[name].values.ravel() if name in fp else np.nan\n",
    "\n",
    "# Build a Pandas DataFrame\n",
    "SWOT_Points = pd.DataFrame({\n",
    "    \"longitude\": getvar(\"longitude\"),\n",
    "    \"latitude\": getvar(\"latitude\"),\n",
    "    \"height\": getvar(\"height\"),\n",
    "    \"phase_noise_std\": getvar(\"phase_noise_std\"),\n",
    "    \"dheight\": getvar(\"dheight_dphase\"),\n",
    "    \"class\": getvar(\"classification\"),\n",
    "    \"classqual\": getvar(\"classification\"),\n",
    "    \"bright_land_flag\": getvar(\"bright_land_flag\"),\n",
    "    \"ancillary_surface_classification_flag\": getvar(\"ancillary_surface_classification_flag\"),\n",
    "    \"waterfrac\": getvar(\"water_frac\"),\n",
    "    \"waterfrac_uncert\": getvar(\"water_frac_uncert\"),\n",
    "    \"prior_water_prob\": getvar(\"prior_water_prob\"),\n",
    "    \"geolocqual\": getvar(\"geolocation_qual\"),\n",
    "    \"sig0\": getvar(\"sig0\"),\n",
    "    \"sig0_uncert\": getvar(\"sig0_uncert\"),\n",
    "    \"sig0_qual\": getvar(\"sig0_qual\"),\n",
    "    \"crosstrack\": getvar(\"cross_track\"),\n",
    "    \"pixel_area\": getvar(\"pixel_area\"),\n",
    "    \"darea_dheight\": getvar(\"darea_dheight\"),\n",
    "    # elevation corrections\n",
    "    \"geoid\": getvar(\"geoid\"),\n",
    "    \"solid_tide\": getvar(\"solid_earth_tide\"),\n",
    "    \"load_tide\": getvar(\"load_tide_fes\"),\n",
    "    \"pole_tide\": getvar(\"pole_tide\")\n",
    "})\n",
    "\n",
    "# Derive elevation & height uncertainty\n",
    "SWOT_Points[\"height_uncert\"] = SWOT_Points[\"phase_noise_std\"] * SWOT_Points[\"dheight\"]\n",
    "SWOT_Points[\"geoid_correction\"] = SWOT_Points[\"geoid\"] - SWOT_Points[\"solid_tide\"] - SWOT_Points[\"load_tide\"] - SWOT_Points[\"pole_tide\"]\n",
    "SWOT_Points[\"elevation\"] = SWOT_Points[\"height\"] - SWOT_Points[\"geoid_correction\"]\n",
    "\n",
    "# Drop any empty rows\n",
    "SWOT_Points = SWOT_Points.dropna(how=\"all\")\n",
    "\n",
    "# Quality filtering\n",
    "geolocqual_problem_bits = {\n",
    "    4, 4101, 5, 6, 4100, 4102, 524292, 524293, 524294, 524295,\n",
    "    528389, 528390, 7, 528388, 16777220, 17301508, 17305604,\n",
    "    528391, 4103\n",
    "}\n",
    "\n",
    "SWOT_Points = SWOT_Points[\n",
    "    (~SWOT_Points[\"geolocqual\"].isin(geolocqual_problem_bits)) &\n",
    "    (SWOT_Points[\"crosstrack\"].abs().between(10000, 60000))]\n",
    "\n",
    "print(SWOT_Points.head())\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add metadata attributes\n",
    "\n",
    "# Get the attributes of interest from the data_structure\n",
    "cycle_number = data_structure.cycle_number\n",
    "pass_number = data_structure.pass_number\n",
    "tile_number = data_structure.tile_number\n",
    "time_granule_start = data_structure.time_granule_start\n",
    "\n",
    "# Add these as new columns to the SWOT_Points DataFrame\n",
    "SWOT_Points[\"cycle_number\"] = cycle_number\n",
    "SWOT_Points[\"pass_number\"] = pass_number\n",
    "SWOT_Points[\"tile_number\"] = tile_number\n",
    "SWOT_Points[\"time\"] = time_granule_start # just taking start time since it's a diff of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add cycle, pass, time to SWOT_Points df\n",
    "# for ML:\n",
    "# longitude, latitude, NORMALIZED to each tile elevation, phase_noise_std, sig0\n",
    "# crosstrack, maybe cycle & pass??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing with initial date filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for fancy progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Sentinel-2 dates\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# File path\n",
    "csv_file_path = 'data/image_dates.csv'\n",
    "\n",
    "# Read CSV and parse 'date' column as datetime\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Filter to dates after July 1, 2025\n",
    "cutoff_date = datetime(2025, 7, 1)\n",
    "date_objs = df[df['date'] > cutoff_date]['date'].tolist()\n",
    "\n",
    "day_tolerance = 1\n",
    "\n",
    "# Display results\n",
    "print(len(date_objs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# setup\n",
    "# ---------------------------\n",
    "base_dir = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data\"\n",
    "pattern = os.path.join(base_dir, \"SWOT_L2_HR_PIXC_*.nc\")\n",
    "\n",
    "# regex to find timestamps like 20250605T070551 in filenames\n",
    "timestamp_re = re.compile(r\"\\d{8}T\\d{6}\")\n",
    "\n",
    "# geolocqual problem bits\n",
    "geolocqual_problem_bits = {\n",
    "    4, 4101, 5, 6, 4100, 4102, 524292, 524293, 524294, 524295,\n",
    "    528389, 528390, 7, 528388, 16777220, 17301508, 17305604,\n",
    "    528391, 4103\n",
    "}\n",
    "\n",
    "# mapping from netCDF variable names to DataFrame column names\n",
    "mapping = {\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"height\": \"height\",\n",
    "    \"phase_noise_std\": \"phase_noise_std\",\n",
    "#     \"dheight_dphase\": \"dheight\",\n",
    "#     \"classification\": \"classification\",\n",
    "#     \"bright_land_flag\": \"bright_land_flag\",\n",
    "#     \"ancillary_surface_classification_flag\": \"ancillary_surface_classification_flag\",\n",
    "    \"water_frac\": \"waterfrac\",\n",
    "#     \"water_frac_uncert\": \"waterfrac_uncert\",\n",
    "#     \"prior_water_prob\": \"prior_water_prob\",\n",
    "    \"geolocation_qual\": \"geolocqual\",\n",
    "    \"sig0\": \"sig0\",\n",
    "    \"sig0_uncert\": \"sig0_uncert\",\n",
    "#     \"sig0_qual\": \"sig0_qual\",\n",
    "    \"cross_track\": \"crosstrack\",\n",
    "#     \"pixel_area\": \"pixel_area\",\n",
    "#     \"darea_dheight\": \"darea_dheight\",\n",
    "    \"geoid\": \"geoid\",\n",
    "    \"solid_earth_tide\": \"solid_tide\",\n",
    "    \"load_tide_fes\": \"load_tide\",\n",
    "    \"pole_tide\": \"pole_tide\"\n",
    "}\n",
    "\n",
    "# Columns to drop computing elevation (attempting to reduce written file size & memory here)\n",
    "cols_to_drop_after_elevation = [\n",
    "    \"geoid_correction\", \"pole_tide\", \"load_tide\", \"solid_tide\", \"geoid\", \"height\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def safe_var_read(group, varname):\n",
    "    \"\"\"Read a variable from a netCDF4 group safely. Return numpy array or None.\"\"\"\n",
    "    if varname not in group.variables:\n",
    "        return None\n",
    "    var = group.variables[varname]\n",
    "    arr = var[:]\n",
    "    if np.ma.isMaskedArray(arr):\n",
    "        arr = arr.filled(np.nan)\n",
    "    return np.asarray(arr)\n",
    "\n",
    "\n",
    "def process_single_file(nc_path, geolocqual_problem_bits=geolocqual_problem_bits):\n",
    "    \"\"\"\n",
    "    Memory-efficient processing: read small variables first, compute mask,\n",
    "    then read only masked indices for other variables.\n",
    "    Returns a DataFrame (or None on error / empty).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with netCDF4.Dataset(nc_path, mode=\"r\") as ds_root:\n",
    "            # access pixel_cloud group if present\n",
    "            grp = ds_root.groups.get(\"pixel_cloud\", ds_root)\n",
    "\n",
    "            # read minimal variables for mask\n",
    "            geolocqual = safe_var_read(grp, \"geolocation_qual\")\n",
    "            crosstrack = safe_var_read(grp, \"cross_track\")\n",
    "\n",
    "            if geolocqual is None or crosstrack is None:\n",
    "                print(f\"Missing geolocqual or crosstrack in {nc_path}; skipping file.\")\n",
    "                return None\n",
    "\n",
    "            geolocqual = np.asarray(geolocqual).ravel()\n",
    "            crosstrack = np.asarray(crosstrack).ravel()\n",
    "\n",
    "            # prepare integer version of geolocqual (NaNs -> sentinel)\n",
    "            valid_mask_geo = ~np.isnan(geolocqual)\n",
    "            geolocqual_int = np.full(geolocqual.shape, -999999, dtype=np.int64)\n",
    "            if valid_mask_geo.any():\n",
    "                geolocqual_int[valid_mask_geo] = geolocqual[valid_mask_geo].astype(np.int64)\n",
    "\n",
    "            mask_geoloc = ~np.isin(geolocqual_int, list(geolocqual_problem_bits))\n",
    "\n",
    "            abs_ct = np.abs(crosstrack)\n",
    "            mask_crosstrack = (abs_ct >= 10000) & (abs_ct <= 60000)\n",
    "\n",
    "            good_mask = mask_geoloc & mask_crosstrack\n",
    "\n",
    "            if not np.any(good_mask):\n",
    "                return None\n",
    "\n",
    "            n_good = int(np.count_nonzero(good_mask))\n",
    "            data = {}\n",
    "\n",
    "            # read each variable but only keep good_mask indices (defensive about missing vars)\n",
    "            for varname, colname in mapping.items():\n",
    "                if varname == \"geolocation_qual\":\n",
    "                    data[colname] = geolocqual[good_mask]\n",
    "                    continue\n",
    "                if varname == \"cross_track\":\n",
    "                    data[colname] = crosstrack[good_mask]\n",
    "                    continue\n",
    "\n",
    "                arr = safe_var_read(grp, varname)\n",
    "                if arr is None:\n",
    "                    data[colname] = np.full(n_good, np.nan)\n",
    "                else:\n",
    "                    arr = np.asarray(arr).ravel()\n",
    "                    if arr.shape[0] < good_mask.shape[0]:\n",
    "                        tmp = np.full(good_mask.shape[0], np.nan)\n",
    "                        tmp[: arr.shape[0]] = arr\n",
    "                        arr = tmp\n",
    "                    data[colname] = arr[good_mask]\n",
    "\n",
    "            # build dataframe and derived columns\n",
    "            SWOT_df = pd.DataFrame(data)\n",
    "\n",
    "            geoid_vals = np.asarray(SWOT_df[\"geoid\"].values, dtype=float)\n",
    "            solid_tide_vals = np.asarray(SWOT_df[\"solid_tide\"].values, dtype=float)\n",
    "            load_tide_vals = np.asarray(SWOT_df[\"load_tide\"].values, dtype=float)\n",
    "            pole_tide_vals = np.asarray(SWOT_df[\"pole_tide\"].values, dtype=float)\n",
    "            SWOT_df[\"geoid_correction\"] = geoid_vals - solid_tide_vals - load_tide_vals - pole_tide_vals\n",
    "\n",
    "            height_vals = np.asarray(SWOT_df[\"height\"].values, dtype=float)\n",
    "            SWOT_df[\"elevation\"] = height_vals - SWOT_df[\"geoid_correction\"].values\n",
    "            \n",
    "            # ---- DROP heavy/intermediate columns BEFORE writing ----\n",
    "            for c in cols_to_drop_after_elevation:\n",
    "                if c in SWOT_df.columns:\n",
    "                    del SWOT_df[c]\n",
    "\n",
    "            SWOT_df = SWOT_df.dropna(how=\"all\")\n",
    "            if SWOT_df.empty:\n",
    "                return None\n",
    "\n",
    "            # attach root-level metadata\n",
    "            def safe_attr(ds, name):\n",
    "                return getattr(ds, name, np.nan)\n",
    "\n",
    "            SWOT_df[\"cycle_number\"] = safe_attr(ds_root, \"cycle_number\")\n",
    "            SWOT_df[\"pass_number\"] = safe_attr(ds_root, \"pass_number\")\n",
    "            SWOT_df[\"tile_number\"] = safe_attr(ds_root, \"tile_number\")\n",
    "            SWOT_df[\"time_granule_start\"] = safe_attr(ds_root, \"time_granule_start\")\n",
    "            SWOT_df[\"source_file\"] = os.path.basename(nc_path)\n",
    "\n",
    "            return SWOT_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135 files matching pattern.\n",
      "Found 33 timestamps within +/-1 days of targets.\n",
      "Filtered timestamps (sample): ['20250701T105509', '20250706T021314', '20250707T021355', '20250717T003602', '20250718T003643', '20250718T003653', '20250719T003714', '20250719T003724', '20250719T003734', '20250719T073822']\n",
      "Filtered to 36 files to process.\n",
      "Unique days in filtered timestamps: ['20250701', '20250706', '20250707', '20250717', '20250718', '20250719', '20250720', '20250721', '20250726', '20250727', '20250728', '20250731', '20250801', '20250806', '20250807', '20250808', '20250811', '20250812', '20250816', '20250817']\n",
      "Total unique days: 20\n",
      "\n",
      "[1/36] Processing: SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_20250701T105520_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [00:00<00:28,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_20250701T105520_PID0_01.nc (197238 rows)\n",
      "\n",
      "[2/36] Processing: SWOT_L2_HR_PIXC_035_181_278L_20250706T021314_20250706T021325_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in greater_equal\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in less_equal\n",
      "  6%|▌         | 2/36 [00:01<00:23,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_181_278L_20250706T021314_20250706T021325_PID0_01.nc (79960 rows)\n",
      "\n",
      "[3/36] Processing: SWOT_L2_HR_PIXC_035_209_279R_20250707T021355_20250707T021406_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/36 [00:01<00:18,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_209_279R_20250707T021355_20250707T021406_PID0_01.nc (52322 rows)\n",
      "\n",
      "[4/36] Processing: SWOT_L2_HR_PIXC_035_487_278R_20250717T003602_20250717T003613_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/36 [00:01<00:16,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_487_278R_20250717T003602_20250717T003613_PID0_01.nc (51956 rows)\n",
      "\n",
      "[5/36] Processing: SWOT_L2_HR_PIXC_035_515_279L_20250718T003643_20250718T003654_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [00:02<00:18,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_515_279L_20250718T003643_20250718T003654_PID0_01.nc (156619 rows)\n",
      "\n",
      "[6/36] Processing: SWOT_L2_HR_PIXC_035_515_280L_20250718T003653_20250718T003704_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [00:03<00:18,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_515_280L_20250718T003653_20250718T003704_PID0_01.nc (175026 rows)\n",
      "\n",
      "[7/36] Processing: SWOT_L2_HR_PIXC_035_543_279L_20250719T003714_20250719T003725_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/36 [00:04<00:18,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_543_279L_20250719T003714_20250719T003725_PID0_01.nc (159969 rows)\n",
      "\n",
      "[8/36] Processing: SWOT_L2_HR_PIXC_035_543_280R_20250719T003724_20250719T003735_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [00:04<00:19,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_543_280R_20250719T003724_20250719T003735_PID0_01.nc (135038 rows)\n",
      "\n",
      "[9/36] Processing: SWOT_L2_HR_PIXC_035_543_281R_20250719T003734_20250719T003744_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [00:05<00:19,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_543_281R_20250719T003734_20250719T003744_PID0_01.nc (207151 rows)\n",
      "\n",
      "[10/36] Processing: SWOT_L2_HR_PIXC_035_552_028R_20250719T073822_20250719T073832_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [00:06<00:16,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_552_028R_20250719T073822_20250719T073832_PID0_01.nc (99673 rows)\n",
      "\n",
      "[11/36] Processing: SWOT_L2_HR_PIXC_035_552_031R_20250719T073851_20250719T073902_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [00:06<00:13,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_552_031R_20250719T073851_20250719T073902_PID0_01.nc (46199 rows)\n",
      "\n",
      "[12/36] Processing: SWOT_L2_HR_PIXC_035_571_280R_20250720T003755_20250720T003806_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 12/36 [00:06<00:12,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_571_280R_20250720T003755_20250720T003806_PID0_01.nc (107186 rows)\n",
      "\n",
      "[13/36] Processing: SWOT_L2_HR_PIXC_035_580_029L_20250720T073902_20250720T073913_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [00:07<00:12,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_580_029L_20250720T073902_20250720T073913_PID0_01.nc (111584 rows)\n",
      "\n",
      "[14/36] Processing: SWOT_L2_HR_PIXC_035_580_029R_20250720T073902_20250720T073913_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [00:08<00:12,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_580_029R_20250720T073902_20250720T073913_PID0_01.nc (96461 rows)\n",
      "\n",
      "[15/36] Processing: SWOT_L2_HR_PIXC_035_580_031L_20250720T073922_20250720T073933_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [00:08<00:11,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_035_580_031L_20250720T073922_20250720T073933_PID0_01.nc (68084 rows)\n",
      "\n",
      "[16/36] Processing: SWOT_L2_HR_PIXC_036_024_029L_20250721T073933_20250721T073944_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 16/36 [00:09<00:10,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_024_029L_20250721T073933_20250721T073944_PID0_01.nc (69907 rows)\n",
      "\n",
      "[17/36] Processing: SWOT_L2_HR_PIXC_036_024_029R_20250721T073933_20250721T073944_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [00:09<00:10,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_024_029R_20250721T073933_20250721T073944_PID0_01.nc (167923 rows)\n",
      "\n",
      "[18/36] Processing: SWOT_L2_HR_PIXC_036_024_030L_20250721T073943_20250721T073954_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [00:10<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_024_030L_20250721T073943_20250721T073954_PID0_01.nc (112321 rows)\n",
      "\n",
      "[19/36] Processing: SWOT_L2_HR_PIXC_036_181_278L_20250726T225819_20250726T225830_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [00:10<00:09,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_181_278L_20250726T225819_20250726T225830_PID0_01.nc (79658 rows)\n",
      "\n",
      "[20/36] Processing: SWOT_L2_HR_PIXC_036_209_279R_20250727T225900_20250727T225911_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [00:11<00:07,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_209_279R_20250727T225900_20250727T225911_PID0_01.nc (53283 rows)\n",
      "\n",
      "[21/36] Processing: SWOT_L2_HR_PIXC_036_237_279L_20250728T225930_20250728T225942_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [00:11<00:07,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_237_279L_20250728T225930_20250728T225942_PID0_01.nc (84622 rows)\n",
      "\n",
      "[22/36] Processing: SWOT_L2_HR_PIXC_036_302_029R_20250731T060149_20250731T060201_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [00:12<00:07,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_302_029R_20250731T060149_20250731T060201_PID0_01.nc (165451 rows)\n",
      "\n",
      "[23/36] Processing: SWOT_L2_HR_PIXC_036_302_030L_20250731T060159_20250731T060211_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23/36 [00:13<00:08,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_302_030L_20250731T060159_20250731T060211_PID0_01.nc (136982 rows)\n",
      "\n",
      "[24/36] Processing: SWOT_L2_HR_PIXC_036_330_030L_20250801T060230_20250801T060242_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [00:13<00:07,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_330_030L_20250801T060230_20250801T060242_PID0_01.nc (145907 rows)\n",
      "\n",
      "[25/36] Processing: SWOT_L2_HR_PIXC_036_487_278R_20250806T212105_20250806T212116_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [00:14<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_487_278R_20250806T212105_20250806T212116_PID0_01.nc (47848 rows)\n",
      "\n",
      "[26/36] Processing: SWOT_L2_HR_PIXC_036_515_279L_20250807T212146_20250807T212157_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [00:15<00:06,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_515_279L_20250807T212146_20250807T212157_PID0_01.nc (164324 rows)\n",
      "\n",
      "[27/36] Processing: SWOT_L2_HR_PIXC_036_515_280L_20250807T212156_20250807T212207_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27/36 [00:15<00:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_515_280L_20250807T212156_20250807T212207_PID0_01.nc (182603 rows)\n",
      "\n",
      "[28/36] Processing: SWOT_L2_HR_PIXC_036_543_279L_20250808T212217_20250808T212228_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [00:16<00:05,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_543_279L_20250808T212217_20250808T212228_PID0_01.nc (156913 rows)\n",
      "\n",
      "[29/36] Processing: SWOT_L2_HR_PIXC_036_543_280R_20250808T212227_20250808T212238_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29/36 [00:17<00:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_543_280R_20250808T212227_20250808T212238_PID0_01.nc (120013 rows)\n",
      "\n",
      "[30/36] Processing: SWOT_L2_HR_PIXC_036_543_281R_20250808T212237_20250808T212247_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 30/36 [00:18<00:04,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_036_543_281R_20250808T212237_20250808T212247_PID0_01.nc (163933 rows)\n",
      "\n",
      "[31/36] Processing: SWOT_L2_HR_PIXC_037_024_029L_20250811T042437_20250811T042448_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [00:18<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_024_029L_20250811T042437_20250811T042448_PID0_01.nc (104752 rows)\n",
      "\n",
      "[32/36] Processing: SWOT_L2_HR_PIXC_037_024_029R_20250811T042437_20250811T042448_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32/36 [00:19<00:02,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_024_029R_20250811T042437_20250811T042448_PID0_01.nc (219996 rows)\n",
      "\n",
      "[33/36] Processing: SWOT_L2_HR_PIXC_037_024_030L_20250811T042447_20250811T042458_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [00:19<00:01,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_024_030L_20250811T042447_20250811T042458_PID0_01.nc (145709 rows)\n",
      "\n",
      "[34/36] Processing: SWOT_L2_HR_PIXC_037_052_030L_20250812T042518_20250812T042529_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 34/36 [00:20<00:01,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_052_030L_20250812T042518_20250812T042529_PID0_01.nc (213648 rows)\n",
      "\n",
      "[35/36] Processing: SWOT_L2_HR_PIXC_037_181_278L_20250816T194323_20250816T194334_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 35/36 [00:21<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_181_278L_20250816T194323_20250816T194334_PID0_01.nc (81799 rows)\n",
      "\n",
      "[36/36] Processing: SWOT_L2_HR_PIXC_037_209_279R_20250817T194404_20250817T194415_PID0_01.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:21<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: SWOT_L2_HR_PIXC_037_209_279R_20250817T194404_20250817T194415_PID0_01.nc (61006 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (4423064, 14)\n",
      "    longitude   latitude  phase_noise_std  waterfrac  geolocqual        sig0  \\\n",
      "0 -149.552404  69.236015         0.045659   0.167256           0   24.465372   \n",
      "1 -149.551882  69.236147         0.031159   1.224904           0  322.846558   \n",
      "2 -149.551167  69.236328         0.026563   2.583732           0  965.866943   \n",
      "3 -149.550266  69.236555         0.022460   1.348676           0  588.309814   \n",
      "4 -149.549266  69.236808         0.023962   0.945739           0  326.267853   \n",
      "\n",
      "   sig0_uncert    crosstrack   elevation  cycle_number  pass_number  \\\n",
      "0    11.847507 -13393.489258  296.291781            35           52   \n",
      "1   151.669876 -13441.602539  295.972732            35           52   \n",
      "2   452.993774 -13492.628906  295.815024            35           52   \n",
      "3   276.064575 -13541.678711  295.813125            35           52   \n",
      "4   153.266998 -13596.972656  295.896783            35           52   \n",
      "\n",
      "   tile_number           time_granule_start  \\\n",
      "0           30  2025-07-01T10:55:09.367570Z   \n",
      "1           30  2025-07-01T10:55:09.367570Z   \n",
      "2           30  2025-07-01T10:55:09.367570Z   \n",
      "3           30  2025-07-01T10:55:09.367570Z   \n",
      "4           30  2025-07-01T10:55:09.367570Z   \n",
      "\n",
      "                                         source_file  \n",
      "0  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "1  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "2  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "3  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n",
      "4  SWOT_L2_HR_PIXC_035_052_030L_20250701T105509_2...  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Main!!! find files, extract timestamps, filter, process\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # find all files\n",
    "    all_files = sorted(glob.glob(pattern))\n",
    "    print(f\"Found {len(all_files)} files matching pattern.\")\n",
    "\n",
    "    # extract timestamps from filenames\n",
    "    file_to_ts = {}\n",
    "    for f in all_files:\n",
    "        m = timestamp_re.search(os.path.basename(f))\n",
    "        if m:\n",
    "            ts = m.group()  # e.g., \"20250605T070551\"\n",
    "            file_to_ts[f] = ts\n",
    "\n",
    "    # filter timestamps to those within +/- day_tolerance of any target date\n",
    "    filtered_timestamps = []\n",
    "    for ts in sorted(set(file_to_ts.values())):\n",
    "        ts_date = datetime.strptime(ts[:8], \"%Y%m%d\")\n",
    "        if any(abs((ts_date - d).days) <= day_tolerance for d in date_objs):\n",
    "            filtered_timestamps.append(ts)\n",
    "\n",
    "    print(f\"Found {len(filtered_timestamps)} timestamps within +/-{day_tolerance} days of targets.\")\n",
    "    if filtered_timestamps:\n",
    "        print(\"Filtered timestamps (sample):\", filtered_timestamps[:10])\n",
    "\n",
    "    # filter files to only those containing a kept timestamp\n",
    "    filtered_files = [f for f, ts in file_to_ts.items() if ts in filtered_timestamps]\n",
    "    print(f\"Filtered to {len(filtered_files)} files to process.\")\n",
    "\n",
    "    # optional: show unique days in filtered_timestamps\n",
    "    unique_days = sorted({ts[:8] for ts in filtered_timestamps})\n",
    "    print(\"Unique days in filtered timestamps:\", unique_days)\n",
    "    print(\"Total unique days:\", len(unique_days))\n",
    "            \n",
    "    dfs = []\n",
    "    for i, f in enumerate(tqdm(filtered_files), 1):\n",
    "        print(f\"\\n[{i}/{len(filtered_files)}] Processing: {os.path.basename(f)}\")\n",
    "        df = process_single_file(f)\n",
    "        if df is not None and not df.empty:\n",
    "            dfs.append(df)\n",
    "            print(f\"Completed: {os.path.basename(f)} ({len(df)} rows)\")\n",
    "        else:\n",
    "            print(f\"Skipping empty/failed result for {os.path.basename(f)}\")\n",
    "\n",
    "    if dfs:\n",
    "        SWOT_Points_all = pd.concat(dfs, ignore_index=True)\n",
    "        print(\"Combined DataFrame shape:\", SWOT_Points_all.shape)\n",
    "    else:\n",
    "        SWOT_Points_all = pd.DataFrame()\n",
    "        print(\"No valid data found in filtered files.\")\n",
    "\n",
    "    # return the DataFrame and some metadata for further use\n",
    "    return {\n",
    "        \"SWOT_Points_all\": SWOT_Points_all,\n",
    "        \"filtered_files\": filtered_files,\n",
    "        \"filtered_timestamps\": filtered_timestamps,\n",
    "        \"unique_days\": unique_days\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    # proof that it worked plz\n",
    "    if not results[\"SWOT_Points_all\"].empty:\n",
    "        print(results[\"SWOT_Points_all\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a csv\n",
    "\n",
    "output_csv = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data/filtered_SWOT_PIXC.csv\"\n",
    "\n",
    "results[\"SWOT_Points_all\"].to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a python pickle\n",
    "\n",
    "import pickle\n",
    "output_pkl = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data/filtered_SWOT_PIXC.pkl\"\n",
    "\n",
    "results[\"SWOT_Points_all\"].to_pickle(output_pkl)\n",
    "\n",
    "# this is MUCH faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
