{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWOT PIXC pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One file test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Filepath ----------\n",
    "aufeis_test_tile_filepath = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data/SWOT_L2_HR_PIXC_033_487_278R_20250605T070551_20250605T070602_PID0_01.nc\"\n",
    "\n",
    "# NetCDF structure check\n",
    "import netCDF4\n",
    "data_structure = netCDF4.Dataset(aufeis_test_tile_filepath)\n",
    "# print(data_structure) # shows 'groups: pixel_cloud, tvp, noise'\n",
    "\n",
    "# file pointer\n",
    "fp = xr.open_dataset(aufeis_test_tile_filepath, group = 'pixel_cloud')\n",
    "# print(fp)\n",
    "# print(fp.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      longitude   latitude      height  phase_noise_std   dheight  class  \\\n",
      "211 -143.098029  68.201372  571.506653         0.117613  0.744377    6.0   \n",
      "212 -143.096199  68.200914  571.323730         0.110314  0.758199    6.0   \n",
      "213 -143.093815  68.200317  571.329895         0.131377  0.776198    3.0   \n",
      "285 -142.934672  68.160307  572.359375         0.109845  1.980290    3.0   \n",
      "286 -142.934076  68.160156  572.105469         0.061149  1.984812    3.0   \n",
      "\n",
      "     classqual  bright_land_flag  ancillary_surface_classification_flag  \\\n",
      "211        6.0               0.0                                    1.0   \n",
      "212        6.0               0.0                                    1.0   \n",
      "213        3.0               0.0                                    1.0   \n",
      "285        3.0               0.0                                    1.0   \n",
      "286        3.0               0.0                                    1.0   \n",
      "\n",
      "     waterfrac     ...        crosstrack   pixel_area  darea_dheight  \\\n",
      "211   0.888415     ...      11937.378906  1088.975708       0.005491   \n",
      "212   1.082915     ...      11958.584961  1087.053467       0.005472   \n",
      "213   0.215443     ...      11978.597656  1085.245728       0.005454   \n",
      "285   0.190502     ...      13149.444336   989.058838       0.004536   \n",
      "286   0.441215     ...      13197.599609   985.463013       0.004503   \n",
      "\n",
      "        geoid  solid_tide  load_tide  pole_tide  height_uncert  \\\n",
      "211  6.751453   -0.022910  -0.008105  -0.000049       0.087549   \n",
      "212  6.750676   -0.022910  -0.008105  -0.000049       0.083640   \n",
      "213  6.749667   -0.022911  -0.008105  -0.000049       0.101975   \n",
      "285  6.684452   -0.022986  -0.008123  -0.000052       0.217524   \n",
      "286  6.684213   -0.022986  -0.008123  -0.000052       0.121369   \n",
      "\n",
      "     geoid_correction   elevation  \n",
      "211          6.782516  564.724121  \n",
      "212          6.781741  564.541992  \n",
      "213          6.780733  564.549133  \n",
      "285          6.715611  565.643738  \n",
      "286          6.715374  565.390076  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the variables we want into 1D numpy arrays\n",
    "def getvar(name):\n",
    "    return fp[name].values.ravel() if name in fp else np.nan\n",
    "\n",
    "# Build a Pandas DataFrame\n",
    "SWOT_Points = pd.DataFrame({\n",
    "    \"longitude\": getvar(\"longitude\"),\n",
    "    \"latitude\": getvar(\"latitude\"),\n",
    "    \"height\": getvar(\"height\"),\n",
    "    \"phase_noise_std\": getvar(\"phase_noise_std\"),\n",
    "    \"dheight\": getvar(\"dheight_dphase\"),\n",
    "    \"class\": getvar(\"classification\"),\n",
    "    \"classqual\": getvar(\"classification\"),\n",
    "    \"bright_land_flag\": getvar(\"bright_land_flag\"),\n",
    "    \"ancillary_surface_classification_flag\": getvar(\"ancillary_surface_classification_flag\"),\n",
    "    \"waterfrac\": getvar(\"water_frac\"),\n",
    "    \"waterfrac_uncert\": getvar(\"water_frac_uncert\"),\n",
    "    \"prior_water_prob\": getvar(\"prior_water_prob\"),\n",
    "    \"geolocqual\": getvar(\"geolocation_qual\"),\n",
    "    \"sig0\": getvar(\"sig0\"),\n",
    "    \"sig0_uncert\": getvar(\"sig0_uncert\"),\n",
    "    \"sig0_qual\": getvar(\"sig0_qual\"),\n",
    "    \"crosstrack\": getvar(\"cross_track\"),\n",
    "    \"pixel_area\": getvar(\"pixel_area\"),\n",
    "    \"darea_dheight\": getvar(\"darea_dheight\"),\n",
    "    # elevation corrections\n",
    "    \"geoid\": getvar(\"geoid\"),\n",
    "    \"solid_tide\": getvar(\"solid_earth_tide\"),\n",
    "    \"load_tide\": getvar(\"load_tide_fes\"),\n",
    "    \"pole_tide\": getvar(\"pole_tide\")\n",
    "})\n",
    "\n",
    "# Derive elevation & height uncertainty\n",
    "SWOT_Points[\"height_uncert\"] = SWOT_Points[\"phase_noise_std\"] * SWOT_Points[\"dheight\"]\n",
    "SWOT_Points[\"geoid_correction\"] = SWOT_Points[\"geoid\"] - SWOT_Points[\"solid_tide\"] - SWOT_Points[\"load_tide\"] - SWOT_Points[\"pole_tide\"]\n",
    "SWOT_Points[\"elevation\"] = SWOT_Points[\"height\"] - SWOT_Points[\"geoid_correction\"]\n",
    "\n",
    "# Drop any empty rows\n",
    "SWOT_Points = SWOT_Points.dropna(how=\"all\")\n",
    "\n",
    "# Quality filtering\n",
    "geolocqual_problem_bits = {\n",
    "    4, 4101, 5, 6, 4100, 4102, 524292, 524293, 524294, 524295,\n",
    "    528389, 528390, 7, 528388, 16777220, 17301508, 17305604,\n",
    "    528391, 4103\n",
    "}\n",
    "\n",
    "SWOT_Points = SWOT_Points[\n",
    "    (~SWOT_Points[\"geolocqual\"].isin(geolocqual_problem_bits)) &\n",
    "    (SWOT_Points[\"crosstrack\"].abs().between(10000, 60000))]\n",
    "\n",
    "print(SWOT_Points.head())\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add metadata attributes\n",
    "\n",
    "# Get the attributes of interest from the data_structure\n",
    "cycle_number = data_structure.cycle_number\n",
    "pass_number = data_structure.pass_number\n",
    "tile_number = data_structure.tile_number\n",
    "time_granule_start = data_structure.time_granule_start\n",
    "\n",
    "# Add these as new columns to the SWOT_Points DataFrame\n",
    "SWOT_Points[\"cycle_number\"] = cycle_number\n",
    "SWOT_Points[\"pass_number\"] = pass_number\n",
    "SWOT_Points[\"tile_number\"] = tile_number\n",
    "SWOT_Points[\"time\"] = time_granule_start # just taking start time since it's a diff of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add cycle, pass, time to SWOT_Points df\n",
    "# for ML:\n",
    "# longitude, latitude, NORMALIZED to each tile elevation, phase_noise_std, sig0\n",
    "# crosstrack, maybe cycle & pass??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/137 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 137 files matching pattern.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/137 [00:02<05:25,  2.39s/it]"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OPTIONAL for nicer progress bars; fallback harmless if not installed\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "# directory & filename pattern\n",
    "base_dir = \"../../../../shared_space/SWOT_Aufeis/SWOT_PIXC_data\"\n",
    "pattern = os.path.join(base_dir, \"SWOT_L2_HR_PIXC_*.nc\")\n",
    "\n",
    "# target_crs\n",
    "# target_crs = \"EPSG:32606\"   # UTM zone 6N\n",
    "\n",
    "# same set of geolocqual \"problem bits\" you used\n",
    "geolocqual_problem_bits = {\n",
    "    4, 4101, 5, 6, 4100, 4102, 524292, 524293, 524294, 524295,\n",
    "    528389, 528390, 7, 528388, 16777220, 17301508, 17305604,\n",
    "    528391, 4103\n",
    "}\n",
    "\n",
    "# list of variables to extract from the pixel_cloud group\n",
    "vars_to_extract = [\n",
    "    \"longitude\", \"latitude\", \"height\", \"phase_noise_std\", \"dheight_dphase\",\n",
    "    \"classification\", \"bright_land_flag\", \"ancillary_surface_classification_flag\",\n",
    "    \"water_frac\", \"water_frac_uncert\", \"prior_water_prob\",\n",
    "    \"geolocation_qual\", \"sig0\", \"sig0_uncert\", \"sig0_qual\",\n",
    "    \"cross_track\", \"pixel_area\", \"darea_dheight\",\n",
    "    \"geoid\", \"solid_earth_tide\", \"load_tide_fes\", \"pole_tide\"\n",
    "]\n",
    "\n",
    "# helper to safely get xr.DataArray -> 1D numpy (ravel) or fill with NaNs if missing\n",
    "def xr_getvar(xds, name):\n",
    "    if name in xds:\n",
    "        da = xds[name]\n",
    "        # ensure numeric dtype and flatten (ravel) to 1D\n",
    "        vals = da.values\n",
    "        if np.ma.isMaskedArray(vals):\n",
    "            vals = vals.filled(np.nan)\n",
    "        return vals.ravel()\n",
    "    else:\n",
    "        # return array of NaNs with length equal to first dimension if possible\n",
    "        # but we cannot know the length here, so return np.nan and let pandas handle broadcasting\n",
    "        return np.nan\n",
    "\n",
    "def process_single_file(nc_path, geolocqual_problem_bits=geolocqual_problem_bits):\n",
    "    \"\"\"Process one SWOT PIXC file and return a DataFrame (or None on error).\"\"\"\n",
    "    try:\n",
    "        # read global attrs with netCDF4 (for metadata)\n",
    "        ds_root = netCDF4.Dataset(nc_path)\n",
    "        # open pixel_cloud group with xarray (convenient indexing/array ops)\n",
    "        xr_fp = xr.open_dataset(nc_path, group=\"pixel_cloud\", decode_times=False)\n",
    "\n",
    "        # build data dict for DataFrame\n",
    "        data = {}\n",
    "        # map variables to your intended DataFrame column names (matching your earlier code)\n",
    "        mapping = {\n",
    "            \"longitude\": \"longitude\",\n",
    "            \"latitude\": \"latitude\",\n",
    "            \"height\": \"height\",\n",
    "            \"phase_noise_std\": \"phase_noise_std\",\n",
    "            \"dheight_dphase\": \"dheight\",\n",
    "            \"classification\": \"class\",\n",
    "            \"bright_land_flag\": \"bright_land_flag\",\n",
    "            \"ancillary_surface_classification_flag\": \"ancillary_surface_classification_flag\",\n",
    "            \"water_frac\": \"waterfrac\",\n",
    "            \"water_frac_uncert\": \"waterfrac_uncert\",\n",
    "            \"prior_water_prob\": \"prior_water_prob\",\n",
    "            \"geolocation_qual\": \"geolocqual\",\n",
    "            \"sig0\": \"sig0\",\n",
    "            \"sig0_uncert\": \"sig0_uncert\",\n",
    "            \"sig0_qual\": \"sig0_qual\",\n",
    "            \"cross_track\": \"crosstrack\",\n",
    "            \"pixel_area\": \"pixel_area\",\n",
    "            \"darea_dheight\": \"darea_dheight\",\n",
    "            \"geoid\": \"geoid\",\n",
    "            \"solid_earth_tide\": \"solid_tide\",\n",
    "            \"load_tide_fes\": \"load_tide\",\n",
    "            \"pole_tide\": \"pole_tide\"\n",
    "        }\n",
    "\n",
    "        # extract each variable; result may be np.nan if missing\n",
    "        for xr_name, col_name in mapping.items():\n",
    "            data[col_name] = xr_getvar(xr_fp, xr_name)\n",
    "\n",
    "        # Build DataFrame\n",
    "        SWOT_df = pd.DataFrame(data)\n",
    "\n",
    "        # if phase_noise column missing name mismatch handling:\n",
    "        # your earlier code used \"phase_noise_std\" correctly; keep that\n",
    "        # compute derived fields (guard against missing arrays)\n",
    "        if \"phase_noise_std\" in SWOT_df.columns and \"dheight\" in SWOT_df.columns:\n",
    "            SWOT_df[\"height_uncert\"] = SWOT_df[\"phase_noise_std\"] * SWOT_df[\"dheight\"]\n",
    "        else:\n",
    "            SWOT_df[\"height_uncert\"] = np.nan\n",
    "\n",
    "        # geoid correction and elevation\n",
    "        # ensure columns exist (fill with nan if not)\n",
    "        for c in [\"geoid\", \"solid_tide\", \"load_tide\", \"pole_tide\", \"height\"]:\n",
    "            if c not in SWOT_df.columns:\n",
    "                SWOT_df[c] = np.nan\n",
    "\n",
    "        SWOT_df[\"geoid_correction\"] = (\n",
    "            SWOT_df[\"geoid\"] - SWOT_df[\"solid_tide\"] - SWOT_df[\"load_tide\"] - SWOT_df[\"pole_tide\"]\n",
    "        )\n",
    "        SWOT_df[\"elevation\"] = SWOT_df[\"height\"] - SWOT_df[\"geoid_correction\"]\n",
    "\n",
    "        # Add the classification qual / duplicate columns (you previously used classqual from classification)\n",
    "        if \"class\" in SWOT_df.columns:\n",
    "            SWOT_df[\"classqual\"] = SWOT_df[\"class\"]\n",
    "        else:\n",
    "            SWOT_df[\"class\"] = np.nan\n",
    "            SWOT_df[\"classqual\"] = np.nan\n",
    "\n",
    "        # Drop rows that are completely empty\n",
    "        SWOT_df = SWOT_df.dropna(how=\"all\")\n",
    "\n",
    "        # Apply quality filtering (only if geolocqual and crosstrack exist)\n",
    "        if \"geolocqual\" in SWOT_df.columns and \"crosstrack\" in SWOT_df.columns:\n",
    "            SWOT_df = SWOT_df[\n",
    "                (~SWOT_df[\"geolocqual\"].isin(geolocqual_problem_bits)) &\n",
    "                (SWOT_df[\"crosstrack\"].abs().between(10000, 60000))\n",
    "            ]\n",
    "\n",
    "        # extract root-level metadata and attach as constant columns\n",
    "        # If an attribute is missing, fill with np.nan\n",
    "        def safe_attr(ds, name):\n",
    "            return getattr(ds, name, np.nan)\n",
    "\n",
    "        SWOT_df[\"cycle_number\"] = safe_attr(ds_root, \"cycle_number\")\n",
    "        SWOT_df[\"pass_number\"] = safe_attr(ds_root, \"pass_number\")\n",
    "        SWOT_df[\"tile_number\"] = safe_attr(ds_root, \"tile_number\")\n",
    "        # time_granule_start may be a string; keep original string for traceability\n",
    "        SWOT_df[\"time_granule_start\"] = safe_attr(ds_root, \"time_granule_start\")\n",
    "\n",
    "        # record file info\n",
    "        SWOT_df[\"source_file\"] = os.path.basename(nc_path)\n",
    "        SWOT_df[\"source_filepath\"] = os.path.abspath(nc_path)\n",
    "\n",
    "        # close datasets\n",
    "        xr_fp.close()\n",
    "        ds_root.close()\n",
    "\n",
    "        return SWOT_df\n",
    "\n",
    "    except Exception as e:\n",
    "        # print traceback and return None so the loop can continue\n",
    "        print(f\"Error processing {nc_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        try:\n",
    "            xr_fp.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            ds_root.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "\n",
    "# find all matching files\n",
    "all_files = sorted(glob.glob(pattern))\n",
    "print(f\"Found {len(all_files)} files matching pattern.\")\n",
    "\n",
    "# process each file and gather DataFrames\n",
    "dfs = []\n",
    "for f in tqdm(all_files):\n",
    "    df = process_single_file(f)\n",
    "    if df is not None and not df.empty:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping empty/failed result for {f}\")\n",
    "\n",
    "# Concatenate all into one master DataFrame\n",
    "if dfs:\n",
    "    SWOT_Points_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Combined DataFrame shape:\", SWOT_Points_all.shape)\n",
    "else:\n",
    "    SWOT_Points_all = pd.DataFrame()\n",
    "    print(\"No valid data found in files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
